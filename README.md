# Learning_rate
## - manual_step_learning_rate (adam_optimizer)
![step_p](https://sun9-59.userapi.com/c857424/v857424698/12de38/1ReXYQv0bew.jpg) 
![step_l](https://sun9-21.userapi.com/c857424/v857424698/12de42/nOJG3OpGvsw.jpg) 

## - exponential_decay_learning_rate (adam_optimizer)
![exp_p](https://sun9-11.userapi.com/c857424/v857424698/12de69/NWxMQ9NHfbQ.jpg) 
![exp_l](https://sun9-42.userapi.com/c857424/v857424698/12de8d/J_TiOet9Ig4.jpg) 

## - constant_learning_rate (adam_optimizer)
![const_p](https://sun9-3.userapi.com/c857424/v857424698/12deab/a69v_dbzvCw.jpg) 
![const_l](https://sun9-6.userapi.com/c857424/v857424698/12deb5/9DWzr5xKmvM.jpg) 

# Optimizer
## - stochastic gradient descent (without_accumulation)
![no_a_p](https://sun9-31.userapi.com/c205124/v205124698/176af/kHoDw98nOUI.jpg) 
![no_a_l](https://sun9-36.userapi.com/c205124/v205124698/176cd/HjUDF1QC_f4.jpg) 

## - adam_optimiser
![adam_p](https://sun9-31.userapi.com/c205124/v205124698/176af/kHoDw98nOUI.jpg) 
![adam_l](https://sun9-36.userapi.com/c205124/v205124698/176cd/HjUDF1QC_f4.jpg)

